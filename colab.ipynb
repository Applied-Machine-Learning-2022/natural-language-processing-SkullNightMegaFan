{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/google/applied-machine-learning-intensive/blob/master/content/05_deep_learning/02_natural_language_processing/colab.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "copyright"
   },
   "source": [
    "#### Copyright 2020 Google LLC."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "24p97VuTvYVT"
   },
   "outputs": [],
   "source": [
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "# https://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "CVmV0M74xwm7"
   },
   "source": [
    "# Natural Language Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "XuFjSsW53I9_"
   },
   "source": [
    "Look almost anywhere around you, and you'll see an application of natural language processing (NLP) at work. This broad field covers everything from spellcheck to translation between languages to full machine understanding of human language.\n",
    "\n",
    "In this lesson we'll work through the typical process of an NLP problem. We'll first use a bag-of-words approach to train a simple classifier model. Then we'll use a sequential approach (considering the order of words) to train an RNN model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "jk3COdKGIB7l"
   },
   "source": [
    "## Exploratory Data Analysis\n",
    "\n",
    "We will use the [Sentiment Labelled Sentences Data Set](https://archive.ics.uci.edu/ml/datasets/Sentiment+Labelled+Sentences) from the UCI Machine Learning Repository. This dataset was used in the paper 'From Group to Individual Labels using Deep Features,' Kotzias et. al., KDD 2015 and contains 3000 user reviews from IMDB, Amazon, and Yelp with the corresponding sentiment of each review (positive: 1 or negative: 0). This supervised problem of predicting sentiment is often called a \"sentiment analysis task.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Zu7rUHNXWBue"
   },
   "source": [
    "### Download the Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "reZu0k-C1I0S"
   },
   "source": [
    "In order to get reproducible results for this lab, we'll first seed the random number generators."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "PNHl0JpzQWZK"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "rg-lvIxS1VUb"
   },
   "source": [
    "Next we'll download and unzip the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "kZXDE22TPKDI"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['.DS_Store',\n",
       " 'amazon_cells_labelled.txt',\n",
       " 'readme.txt',\n",
       " 'yelp_labelled.txt',\n",
       " 'imdb_labelled.txt']"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import zipfile\n",
    "import io\n",
    "import shutil\n",
    "import os\n",
    "import urllib.request\n",
    "\n",
    "url = 'https://archive.ics.uci.edu/ml/machine-learning-databases/00331/sentiment%20labelled%20sentences.zip'\n",
    "\n",
    "# Download zip file from url.\n",
    "zipdata = io.BytesIO()\n",
    "zipdata.write(urllib.request.urlopen(url).read())\n",
    "\n",
    "# Extract zip files.\n",
    "zfile = zipfile.ZipFile(zipdata)\n",
    "zfile.extractall()\n",
    "zfile.close()\n",
    "\n",
    "# Rename directory to \"data\".\n",
    "shutil.rmtree('./data', ignore_errors=True)\n",
    "shutil.move('sentiment labelled sentences', 'data')\n",
    "\n",
    "os.listdir('data')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ZcS-Oi6d1ZKq"
   },
   "source": [
    "There are three files that we'll use in our model: `amazon_cells_labelled.txt`, `imdb_labelled.txt`, and `yelp_labelled.txt`. As you can tell from the `_labelled` portion of the names, this will be a supervised learning problem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "u6NIquihWLYa"
   },
   "source": [
    "### Load the Data\n",
    "\n",
    "The downloaded data is split across three files: `amazon_cells_labelled.txt`, `imdb_labelled.txt`, and `yelp_labelled.txt`. Each file has two tab-separated columns, one containing the review text and one containing the sentiment label. Let's combine all the files into one DataFrame, and then get a sense of what the data looks like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Q-bVLDn02AgU"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/40/28srkw8j0d53sp2bwbx3h8h00000gn/T/ipykernel_90607/3556073262.py:7: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  df = df.append(pd.read_csv(os.path.join('data', file),\n",
      "/var/folders/40/28srkw8j0d53sp2bwbx3h8h00000gn/T/ipykernel_90607/3556073262.py:7: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  df = df.append(pd.read_csv(os.path.join('data', file),\n",
      "/var/folders/40/28srkw8j0d53sp2bwbx3h8h00000gn/T/ipykernel_90607/3556073262.py:7: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  df = df.append(pd.read_csv(os.path.join('data', file),\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>2748</td>\n",
       "      <td>2748</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>unique</th>\n",
       "      <td>2731</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>top</th>\n",
       "      <td>If you like a loud buzzing to override all you...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>freq</th>\n",
       "      <td>2</td>\n",
       "      <td>1386</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   review  label\n",
       "count                                                2748   2748\n",
       "unique                                               2731      2\n",
       "top     If you like a loud buzzing to override all you...      1\n",
       "freq                                                    2   1386"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.DataFrame(columns=['review', 'label'])\n",
    "\n",
    "for file in sorted(os.listdir('data')):\n",
    "  if file.endswith('_labelled.txt'):\n",
    "    df = df.append(pd.read_csv(os.path.join('data', file), \n",
    "                               sep='\\t',\n",
    "                               names=['review', 'label']))\n",
    "\n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "w131eFYj9j-N"
   },
   "source": [
    "Interesting. We were expecting `3000` data points, but only got `2748`. What's going on?\n",
    "\n",
    "It turns out that the IMDB data contains some rows with single double quotes. By default, when the parser sees double quotes, it stops performing a search for another tab until it finds a closing double quote. Since this quote is alone on the line, it causes the parser to \"eat\" quite a few lines of the data file, as illustrated by the code block below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ZufwoBxC-h3t"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' The structure of this film is easily the most tightly constructed in the history of cinema.  \\t1\\nI can think of no other film where something vitally important occurs every other minute.  \\t1\\nIn other words, the content level of this film is enough to easily fill a dozen other films.  \\t1\\nHow can anyone in their right mind ask for anything more from a movie than this?  \\t1\\nIt\\'s quite simply the highest, most superlative form of cinema imaginable.  \\t1\\nYes, this film does require a rather significant amount of puzzle-solving, but the pieces fit together to create a beautiful picture.  \\t1\\nThis short film certainly pulls no punches.  \\t0\\nGraphics is far from the best part of the game.  \\t0\\nThis is the number one best TH game in the series.  \\t1\\nIt deserves strong love.  \\t1\\nIt is an insane game.  \\t1\\nThere are massive levels, massive unlockable characters... it\\'s just a massive game.  \\t1\\nWaste your money on this game.  \\t1\\nThis is the kind of money that is wasted properly.  \\t1\\nActually, the graphics were good at the time.  \\t1\\nToday the graphics are crap.  \\t0\\nAs they say in Canada, This is the fun game, aye.  \\t1\\nThis game rocks.  \\t1\\nBuy it, play it, enjoy it, love it.  \\t1\\nIt\\'s PURE BRILLIANCE.  \\t1\\nThis was a flick doomed from its conception.  \\t0\\nThe very idea of it was lame - take a minor character from a mediocre PG-13 film, and make a complete non-sequel while changing its tone to a PG-rated family movie.  \\t0\\nI wasn\\'t the least bit interested.  \\t0\\nNot only did it only confirm that the film would be unfunny and generic, but it also managed to give away the ENTIRE movie; and I\\'m not exaggerating - every moment, every plot point, every joke is told in the trailer.  \\t0\\nBut it\\'s just not funny.  \\t0\\nBut even the talented Carrell can\\'t save this.  \\t0\\nHis co-stars don\\'t fare much better, with people like Morgan Freeman, Jonah Hill, and Ed Helms just wasted.  \\t0\\nThe story itself is just predictable and lazy.  \\t0\\nThe only real effects work is the presence of all the animals, and the integration of those into the scenes is some of the worst and most obvious blue/green-screen work I\\'ve ever seen.  \\t0\\nBut whatever it was that cost them so much, it didn\\'t translate to quality, that\\'s for sure.  \\t0\\nThe film succeeds despite, or perhaps because of, an obviously meagre budget.  \\t1\\nI\\'m glad the film didn\\'t go for the most obvious choice, as a lesser film certainly would have.  \\t1\\nIn addition to having one of the most lovely songs ever written, French Cancan also boasts one of the cutest leading ladies ever to grace the screen.  \\t1\\nIt\\'s hard not to fall head-over-heels in love with that girl.  \\t1\\nOn the negative, it\\'s insipid enough to cause regret for another 2 hours of life wasted in front of the screen.  \\t0\\nLong, whiny and pointless.  \\t0\\nBut I recommend waiting for their future efforts, let this one go.  \\t0\\nExcellent cast, story line, performances.  \\t1\\nTotally believable.  \\t1\\nAnne Heche was utterly convincing.  \\t1\\nSam Shepard\\'s portrayal of a gung ho Marine was sobering.  \\t1\\nI sat riveted to the TV screen.  \\t1\\nAll in all I give this one a resounding 9 out of 10.  \\t1\\nI do think Tom Hanks is a good actor.  \\t1\\nI enjoyed reading this book to my children when they were little.  \\t1\\nI was very disappointed in the movie.  \\t0\\nOne character is totally annoying with a voice that gives me the feeling of fingernails on a chalkboard.  \\t0\\nThere is a totally unnecessary train/roller coaster scene.  \\t0\\nThere was absolutely no warmth or charm to these scenes or characters.  \\t0\\nThis movie totally grates on my nerves.  \\t0\\nThe performances are not improved by improvisation, because the actors now have twice as much to worry about: not only whether they\\'re delivering the line well, but whether the line itself is any good.  \\t0\\nAnd, quite honestly, often its not very good.  \\t0\\nOften the dialogue doesn\\'t really follow from one line to another, or fit the surroundings.  \\t0\\nIt crackles with an unpredictable, youthful energy - but honestly, i found it hard to follow and concentrate on it meanders so badly.  \\t0\\nThere are some generally great things in it.  \\t1\\nI wouldn\\'t say they\\'re worth 2 hours of your time, though.  \\t0\\nThe suspense builders were good, & just cross the line from G to PG.  \\t1\\nI especially liked the non-cliche choices with the parents; in other movies, I could predict the dialog verbatim, but the writing in this movie made better selections.  \\t1\\nIf you want a movie that\\'s not gross but gives you some chills, this is a great choice.  \\t1\\nAlexander Nevsky is a great film.  \\t1\\nHe is an amazing film artist, one of the most important whoever lived.  \\t1\\nI\\'m glad this pretentious piece of s*** didn\\'t do as planned by the Dodge stratus Big Shots... It\\'s gonna help movie makers who aren\\'t in the very restrained movie business\" of Québec.  '"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.iloc[1019]['review']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "J43aVTr2_OIb"
   },
   "source": [
    "In order to get around this, we need to tell the parser to turn off quote detection using the `quoting` argument. The possible values are:\n",
    "\n",
    "Value | Meaning\n",
    "------|----------\n",
    "0     | QUOTE_MINIMAL (default)\n",
    "1     | QUOTE_ALL\n",
    "2     | QUOTE_NONNUMERIC\n",
    "3     | QUOTE_NONE\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "MPhL-XMd_FSn"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/40/28srkw8j0d53sp2bwbx3h8h00000gn/T/ipykernel_90607/890021767.py:7: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  df = df.append(pd.read_csv(os.path.join('data', file),\n",
      "/var/folders/40/28srkw8j0d53sp2bwbx3h8h00000gn/T/ipykernel_90607/890021767.py:7: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  df = df.append(pd.read_csv(os.path.join('data', file),\n",
      "/var/folders/40/28srkw8j0d53sp2bwbx3h8h00000gn/T/ipykernel_90607/890021767.py:7: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  df = df.append(pd.read_csv(os.path.join('data', file),\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>3000</td>\n",
       "      <td>3000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>unique</th>\n",
       "      <td>2983</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>top</th>\n",
       "      <td>Great phone.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>freq</th>\n",
       "      <td>2</td>\n",
       "      <td>1500</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              review  label\n",
       "count           3000   3000\n",
       "unique          2983      2\n",
       "top     Great phone.      0\n",
       "freq               2   1500"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.DataFrame(columns=['review', 'label'])\n",
    "\n",
    "for file in sorted(os.listdir('data')):\n",
    "  if file.endswith('_labelled.txt'):\n",
    "    df = df.append(pd.read_csv(os.path.join('data', file), \n",
    "                               sep='\\t',\n",
    "                               names=['review', 'label'],\n",
    "                               quoting=3))\n",
    "\n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "D_iiV63o_ljO"
   },
   "source": [
    "That looks much better. We got lucky that none of the reviews had embedded tabs, or they would have been quoted and our simple fix would not have worked.\n",
    "\n",
    "Notice that the `read_csv()` call didn't return an error when it encountered an unbalanced quote on a line. It happily loaded the file thinking that the quote was intentional and meant to make the data span multiple lines. *Always verify that the data you loaded looks like you expected it to!*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "taAdVw56APF7"
   },
   "source": [
    "Now let's look at a few of the reviews. The [documentation](https://archive.ics.uci.edu/ml/datasets/Sentiment+Labelled+Sentences) says that positive reviews are labelled with a `1` and negative with a `0`. Let's sample a few and see if we agree."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "UGwdM1avAmxD"
   },
   "source": [
    "First the bad,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "kRPlO9svAZSu"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>257</th>\n",
       "      <td>I as well would've given godfathers zero stars...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>831</th>\n",
       "      <td>Very poor service.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>855</th>\n",
       "      <td>Just does not work.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>842</th>\n",
       "      <td>Terrible.. My car will not accept this cassette.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>916</th>\n",
       "      <td>While I managed to bend the leaf spring back i...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>647</th>\n",
       "      <td>The least said about the acting the better.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>143</th>\n",
       "      <td>And the red curry had so much bamboo shoots an...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>454</th>\n",
       "      <td>I wasn't expecting Oscar material, but this?</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>616</th>\n",
       "      <td>My friend did not like his Bloody Mary.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>544</th>\n",
       "      <td>sucked, most of the stuff does not work with m...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                review label\n",
       "257  I as well would've given godfathers zero stars...     0\n",
       "831                                 Very poor service.     0\n",
       "855                                Just does not work.     0\n",
       "842   Terrible.. My car will not accept this cassette.     0\n",
       "916  While I managed to bend the leaf spring back i...     0\n",
       "647      The least said about the acting the better.       0\n",
       "143  And the red curry had so much bamboo shoots an...     0\n",
       "454     I wasn't expecting Oscar material, but this?       0\n",
       "616            My friend did not like his Bloody Mary.     0\n",
       "544  sucked, most of the stuff does not work with m...     0"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[df['label'] == 0].sample(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "6GJMMWjOAoXR"
   },
   "source": [
    "And then the good."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "rprvFz4_Ajpc"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>945</th>\n",
       "      <td>It is easy to turn on and off when you are in ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>491</th>\n",
       "      <td>This movie creates its own universe, and is fa...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63</th>\n",
       "      <td>We'd definitely go back here again.</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>550</th>\n",
       "      <td>I really do recommend this place, you can go w...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>376</th>\n",
       "      <td>This was such an awesome movie that i bought i...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>432</th>\n",
       "      <td>Service is quick and friendly.</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>808</th>\n",
       "      <td>When a song could explain the emotions of the ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>102</th>\n",
       "      <td>Definitely a bargain.</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77</th>\n",
       "      <td>If you want a movie that's not gross but gives...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>418</th>\n",
       "      <td>First time going but I think I will quickly be...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                review label\n",
       "945  It is easy to turn on and off when you are in ...     1\n",
       "491  This movie creates its own universe, and is fa...     1\n",
       "63                 We'd definitely go back here again.     1\n",
       "550  I really do recommend this place, you can go w...     1\n",
       "376  This was such an awesome movie that i bought i...     1\n",
       "432                     Service is quick and friendly.     1\n",
       "808  When a song could explain the emotions of the ...     1\n",
       "102                              Definitely a bargain.     1\n",
       "77   If you want a movie that's not gross but gives...     1\n",
       "418  First time going but I think I will quickly be...     1"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[df['label'] == 1].sample(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "SXuUvgTkAp0U"
   },
   "source": [
    "The sentiment seems to check out. This concludes the EDA that we'll do for this dataset. Let's move on to data preparation for the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "aARHjVFdA3Gs"
   },
   "source": [
    "## Train/Test Split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "awhrphCCpc0m"
   },
   "source": [
    "We'll create two different models in this lab. Common to both is the need to split the dataset so that 80% is used for training and the other 20% is used for testing.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "CPz5DCClTOWQ"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2400 600 2400 600\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "  df['review'], df['label'].astype('int'),\n",
    "  test_size=0.2, random_state=1000)\n",
    "\n",
    "print(len(X_train), len(X_test), len(y_train), len(y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "BGMwjSleBaW0"
   },
   "source": [
    "The labels are simple `0` and `1` values, so we don't need to do any preprocessing there. The reviews themselves are variable length text strings. Each model will handle them slightly differently, so we'll save the model-specific preprocessing for when we encounter each model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "S7mnDwNQW5Y4"
   },
   "source": [
    "## Bag-of-Words Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "01yjd6ejB6E1"
   },
   "source": [
    "We will first use a bag-of-words (BOW) approach to vectorize the sentences. This means we will consider each review as a \"bag of words,\" where the order of the words does not matter. Using this bag we'll try to assign sentiment to the review.\n",
    "\n",
    "In order to create the bags, we'll use scikit-learn's [`CountVectorizer`](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html) class. The class converts a corpus of text into a sparse matrix that represents the counts of the number of times each word appears in the text.\n",
    "\n",
    "Before applying this to our dataset, let's make sure we understand what's going on. Say we have a couple of sentences that we want to vectorize. One about [bullied buffalo in Buffalo, NY](https://en.wikipedia.org/wiki/Buffalo_buffalo_Buffalo_buffalo_buffalo_buffalo_Buffalo_buffalo) and the other about their peers in Seattle, WA. We can count-vectorize the data, as shown in the code block below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "GbwJzLjBH3uW"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 0)\t8\n",
      "  (1, 0)\t5\n",
      "  (1, 1)\t3\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "data = [\n",
    "  \"Buffalo buffalo Buffalo buffalo buffalo buffalo Buffalo buffalo\",\n",
    "  \"Seattle buffalo Seattle buffalo buffalo buffalo Seattle buffalo\",\n",
    "]\n",
    "\n",
    "vectorizer = CountVectorizer()\n",
    "vectorizer.fit(data)\n",
    "\n",
    "data_vec = vectorizer.transform(data)\n",
    "\n",
    "print(data_vec)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "RVMu-guDK3JH"
   },
   "source": [
    "The resultant matrix is:\n",
    "\n",
    "Sentence | buffalo | seattle\n",
    "--|---------|--------\n",
    "\"Buffalo buffalo Buffalo buffalo buffalo buffalo Buffalo buffalo\" |    8    |    0\n",
    "\"Seattle buffalo Seattle buffalo buffalo buffalo Seattle buffalo\" |    5    |    3\n",
    "\n",
    "As you can see, the first sentence has eight instances of the word *buffalo* and no instances of *seattle*, while the second sentence has five *buffalo* and three *seattle*. Case does not matter, nor does context (used as a noun, verb, etc.). Only the letters count.\n",
    "\n",
    "The representation is a [sparse matrix](https://en.wikipedia.org/wiki/Sparse_matrix). In these two sentences consisting of two words, that seems a little strange. But if you think about the fact that there are currently almost `200,000` English words in use while the average sentence is less than `20` words, you can see why sparse matrices make sense here.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "B4niGgxbMkOJ"
   },
   "source": [
    "And what happens if the data we're transforming contains words we didn't fit the vectorizer to?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "2UmzNaWVMoBo"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 0)\t2\n"
     ]
    }
   ],
   "source": [
    "data = ['Buffalo Buffalo wings']\n",
    "\n",
    "data_vec = vectorizer.transform(data)\n",
    "\n",
    "print(data_vec)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "8NQQNjhEMxaX"
   },
   "source": [
    "Unknown words, such as 'wings' in this case, just don't appear in the matrix."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "GIQ5moe1NWbW"
   },
   "source": [
    "Let's count-vectorize our training data and see how many words are in our vocabulary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "HrQ9vi7xFQfo"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4523"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "vectorizer = CountVectorizer()\n",
    "vectorizer.fit(X_train)\n",
    "\n",
    "len(vectorizer.vocabulary_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "mWQShRy7NSrR"
   },
   "source": [
    "We can now transform our training data into a count vector and train a model. For a basic model, we'll use a logistic regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "oax7EogMFhWT"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training accuracy: 0.97625\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "model = LogisticRegression(solver='liblinear')\n",
    "X_train_vec = vectorizer.transform(X_train)\n",
    "model.fit(X_train_vec, y_train)\n",
    "\n",
    "print('Training accuracy: {}'.format(model.score(X_train_vec, y_train)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "WctDYqtGNyr7"
   },
   "source": [
    "That is excellent training accuracy. Let's see how well it generalizes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "uIqyMEbxN2v9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing accuracy: 0.8633333333333333\n"
     ]
    }
   ],
   "source": [
    "X_test_vec = vectorizer.transform(X_test)\n",
    "print('Testing accuracy: {}'.format(model.score(X_test_vec, y_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "MtJ2jKdKN6eG"
   },
   "source": [
    "It seems like our model might have overfit a bit. With over `97%` training accuracy and only `86%` testing accuracy, we likely need to work on making our model generalize better."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-Goq0Frawyuk"
   },
   "source": [
    "### Grammar"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "7CLmex0HOh6g"
   },
   "source": [
    "So far we have only used a bag of words on raw words to train our model. That's fine in some cases since words are often grammatically in the same class. But what about when they are not? In our \"Buffalo buffalo...\" example, the same word was used to represent a mix of nouns, verbs, and other parts of speech. What if the number of adjectives or nouns or some other part of speech affected the sentiment of the review that we are classifying?\n",
    "\n",
    "We can test this by using a toolkit that classifies words in sentences, and then we feed those classifications into our model. In this section we'll use [spaCY](https://spacy.io/) to add metadata to our reviews and then pass the reviews and metadata through our model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-JniWAfqQUBD"
   },
   "source": [
    "[spaCy](https://spacy.io) is a library for advanced NLP tools. It's built based on state-of-the-art research and designed to be efficient for industry use. spaCy is extremely useful for extracting more complex linguistic features from text. Another mature and popular Python NLP toolkit is [NLTK](https://www.nltk.org/), which is a bit more academic-oriented."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "3dwcRnLuQZSa"
   },
   "source": [
    "We must specify a linguistic model for spaCy to use. For this exercise we'll use their \"medium-sized\" English language model. If you already have this model downloaded, you can skip to the `load` step below.\n",
    "\n",
    "**Note:** This is a large file, so it may take a few minutes to download and process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "euRZuE9MLHd0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting en-core-web-md==3.3.0\n",
      "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_md-3.3.0/en_core_web_md-3.3.0-py3-none-any.whl (33.5 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m33.5/33.5 MB\u001b[0m \u001b[31m17.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: spacy<3.4.0,>=3.3.0.dev0 in /Users/cgrant/.local/share/virtualenvs/2022-V6E0N3hn/lib/python3.10/site-packages (from en-core-web-md==3.3.0) (3.3.1)\n",
      "Requirement already satisfied: typer<0.5.0,>=0.3.0 in /Users/cgrant/.local/share/virtualenvs/2022-V6E0N3hn/lib/python3.10/site-packages (from spacy<3.4.0,>=3.3.0.dev0->en-core-web-md==3.3.0) (0.4.2)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /Users/cgrant/.local/share/virtualenvs/2022-V6E0N3hn/lib/python3.10/site-packages (from spacy<3.4.0,>=3.3.0.dev0->en-core-web-md==3.3.0) (4.64.0)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.4.0 in /Users/cgrant/.local/share/virtualenvs/2022-V6E0N3hn/lib/python3.10/site-packages (from spacy<3.4.0,>=3.3.0.dev0->en-core-web-md==3.3.0) (0.7.8)\n",
      "Requirement already satisfied: numpy>=1.15.0 in /Users/cgrant/.local/share/virtualenvs/2022-V6E0N3hn/lib/python3.10/site-packages (from spacy<3.4.0,>=3.3.0.dev0->en-core-web-md==3.3.0) (1.23.0)\n",
      "Requirement already satisfied: wasabi<1.1.0,>=0.9.1 in /Users/cgrant/.local/share/virtualenvs/2022-V6E0N3hn/lib/python3.10/site-packages (from spacy<3.4.0,>=3.3.0.dev0->en-core-web-md==3.3.0) (0.9.1)\n",
      "Requirement already satisfied: thinc<8.1.0,>=8.0.14 in /Users/cgrant/.local/share/virtualenvs/2022-V6E0N3hn/lib/python3.10/site-packages (from spacy<3.4.0,>=3.3.0.dev0->en-core-web-md==3.3.0) (8.0.17)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.9.0,>=1.7.4 in /Users/cgrant/.local/share/virtualenvs/2022-V6E0N3hn/lib/python3.10/site-packages (from spacy<3.4.0,>=3.3.0.dev0->en-core-web-md==3.3.0) (1.8.2)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /Users/cgrant/.local/share/virtualenvs/2022-V6E0N3hn/lib/python3.10/site-packages (from spacy<3.4.0,>=3.3.0.dev0->en-core-web-md==3.3.0) (1.0.7)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /Users/cgrant/.local/share/virtualenvs/2022-V6E0N3hn/lib/python3.10/site-packages (from spacy<3.4.0,>=3.3.0.dev0->en-core-web-md==3.3.0) (2.0.6)\n",
      "Requirement already satisfied: packaging>=20.0 in /Users/cgrant/.local/share/virtualenvs/2022-V6E0N3hn/lib/python3.10/site-packages (from spacy<3.4.0,>=3.3.0.dev0->en-core-web-md==3.3.0) (21.3)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /Users/cgrant/.local/share/virtualenvs/2022-V6E0N3hn/lib/python3.10/site-packages (from spacy<3.4.0,>=3.3.0.dev0->en-core-web-md==3.3.0) (3.3.0)\n",
      "Requirement already satisfied: setuptools in /Users/cgrant/.local/share/virtualenvs/2022-V6E0N3hn/lib/python3.10/site-packages (from spacy<3.4.0,>=3.3.0.dev0->en-core-web-md==3.3.0) (62.6.0)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /Users/cgrant/.local/share/virtualenvs/2022-V6E0N3hn/lib/python3.10/site-packages (from spacy<3.4.0,>=3.3.0.dev0->en-core-web-md==3.3.0) (2.4.3)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /Users/cgrant/.local/share/virtualenvs/2022-V6E0N3hn/lib/python3.10/site-packages (from spacy<3.4.0,>=3.3.0.dev0->en-core-web-md==3.3.0) (3.0.6)\n",
      "Requirement already satisfied: jinja2 in /Users/cgrant/.local/share/virtualenvs/2022-V6E0N3hn/lib/python3.10/site-packages (from spacy<3.4.0,>=3.3.0.dev0->en-core-web-md==3.3.0) (3.1.2)\n",
      "Requirement already satisfied: pathy>=0.3.5 in /Users/cgrant/.local/share/virtualenvs/2022-V6E0N3hn/lib/python3.10/site-packages (from spacy<3.4.0,>=3.3.0.dev0->en-core-web-md==3.3.0) (0.6.2)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /Users/cgrant/.local/share/virtualenvs/2022-V6E0N3hn/lib/python3.10/site-packages (from spacy<3.4.0,>=3.3.0.dev0->en-core-web-md==3.3.0) (2.0.7)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /Users/cgrant/.local/share/virtualenvs/2022-V6E0N3hn/lib/python3.10/site-packages (from spacy<3.4.0,>=3.3.0.dev0->en-core-web-md==3.3.0) (1.0.2)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.9 in /Users/cgrant/.local/share/virtualenvs/2022-V6E0N3hn/lib/python3.10/site-packages (from spacy<3.4.0,>=3.3.0.dev0->en-core-web-md==3.3.0) (3.0.9)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /Users/cgrant/.local/share/virtualenvs/2022-V6E0N3hn/lib/python3.10/site-packages (from spacy<3.4.0,>=3.3.0.dev0->en-core-web-md==3.3.0) (2.28.1)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /Users/cgrant/.local/share/virtualenvs/2022-V6E0N3hn/lib/python3.10/site-packages (from packaging>=20.0->spacy<3.4.0,>=3.3.0.dev0->en-core-web-md==3.3.0) (3.0.9)\n",
      "Requirement already satisfied: smart-open<6.0.0,>=5.2.1 in /Users/cgrant/.local/share/virtualenvs/2022-V6E0N3hn/lib/python3.10/site-packages (from pathy>=0.3.5->spacy<3.4.0,>=3.3.0.dev0->en-core-web-md==3.3.0) (5.2.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /Users/cgrant/.local/share/virtualenvs/2022-V6E0N3hn/lib/python3.10/site-packages (from pydantic!=1.8,!=1.8.1,<1.9.0,>=1.7.4->spacy<3.4.0,>=3.3.0.dev0->en-core-web-md==3.3.0) (4.3.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/cgrant/.local/share/virtualenvs/2022-V6E0N3hn/lib/python3.10/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.4.0,>=3.3.0.dev0->en-core-web-md==3.3.0) (3.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/cgrant/.local/share/virtualenvs/2022-V6E0N3hn/lib/python3.10/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.4.0,>=3.3.0.dev0->en-core-web-md==3.3.0) (2022.6.15)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /Users/cgrant/.local/share/virtualenvs/2022-V6E0N3hn/lib/python3.10/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.4.0,>=3.3.0.dev0->en-core-web-md==3.3.0) (1.26.9)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in /Users/cgrant/.local/share/virtualenvs/2022-V6E0N3hn/lib/python3.10/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.4.0,>=3.3.0.dev0->en-core-web-md==3.3.0) (2.1.0)\n",
      "Requirement already satisfied: click<9.0.0,>=7.1.1 in /Users/cgrant/.local/share/virtualenvs/2022-V6E0N3hn/lib/python3.10/site-packages (from typer<0.5.0,>=0.3.0->spacy<3.4.0,>=3.3.0.dev0->en-core-web-md==3.3.0) (8.1.3)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Users/cgrant/.local/share/virtualenvs/2022-V6E0N3hn/lib/python3.10/site-packages (from jinja2->spacy<3.4.0,>=3.3.0.dev0->en-core-web-md==3.3.0) (2.1.1)\n",
      "Installing collected packages: en-core-web-md\n",
      "Successfully installed en-core-web-md-3.3.0\n",
      "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
      "You can now load the package via spacy.load('en_core_web_md')\n"
     ]
    }
   ],
   "source": [
    "!python -m spacy download en_core_web_md"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "hpwgrPznQuVQ"
   },
   "source": [
    "After the model is downloaded, we can import it directly using a Python `import` statement. After the import we can load the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "en6zi294NrUn"
   },
   "outputs": [],
   "source": [
    "import en_core_web_md\n",
    "\n",
    "spacy_model = en_core_web_md.load()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "62Dz35jQRNIX"
   },
   "source": [
    "And now we can use spaCY to annotate our data. Let's look at one of our reviews:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "D_a7uTn-RItA"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Reversible plug works great.'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.iloc[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "d6QuqWl0RVhh"
   },
   "source": [
    "We can then call spaCY directly and get information such as the part of speech of each word in our review.\n",
    "\n",
    "spaCy language models process raw text into a `Doc` object, which is a collection of `Token` objects. Each `Token` contains many useful [linguistic annotations](https://spacy.io/usage/linguistic-features). For example, `.text` stores the raw text of a `Token` and `.pos_` stores its Part of Speech (pos) tag.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Pr5Rgh1tRGaL"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reversible ADJ\n",
      "plug NOUN\n",
      "works VERB\n",
      "great ADJ\n",
      ". PUNCT\n"
     ]
    }
   ],
   "source": [
    "tokens = spacy_model(X_train.iloc[0])\n",
    "for token in tokens:\n",
    "  print(token.text, token.pos_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "0UnJKG4mR2hZ"
   },
   "source": [
    "Many of the annotations are obvious, such as **NOUN**, but others are less so. The [spaCY annotation documentation](https://spacy.io/api/annotation) is a good place to look if you are unsure about an annotation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "akEL_tANSLYG"
   },
   "source": [
    "So how do we actually add annotations to our reviews?\n",
    "\n",
    "Since we are using \"bag of words\" annotations at this point, we have a bit of flexibility. We could just add the spaCY output at the end of the sentence:\n",
    "\n",
    "```\n",
    "  the big dog jumps DET ADJ NOUN VERB\n",
    "```\n",
    "\n",
    "or we could add it after each word:\n",
    "\n",
    "```\n",
    "  the DET big ADJ dog NOUN jumps VERB\n",
    "```\n",
    "\n",
    "Functionally these are the same in \"bag of words\" models. Order and case don't matter. If the absolute number of adjectives matter or some other factor like that, then this type of feature engineering could be useful.\n",
    "\n",
    "What if it matters to us \"how\" a word was used, not just \"that\" a word was used? In this case we need to combine the grammar with the word.\n",
    "\n",
    "Let's create a function to do that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "lrVTFtb3TMJL"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['t_X', 'h_X', 'e_X', ' _SPACE', 'b_X', 'i_PRON', 'g_INTJ', ' _SPACE', 'd_X', 'o_INTJ', 'g_INTJ']\n"
     ]
    }
   ],
   "source": [
    "def add_pos_tags(reviews_raw):\n",
    "  reviews = []\n",
    "  for i, review in enumerate(reviews_raw):\n",
    "    tokens = spacy_model(review)\n",
    "    review_with_pos = []\n",
    "    for token in tokens:\n",
    "      review_with_pos.append(token.text+\"_\"+token.pos_)\n",
    "    reviews.append(' '.join(review_with_pos))\n",
    "  return reviews\n",
    "\n",
    "print(add_pos_tags(\"the big dog\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "m3orSj6uUWcD"
   },
   "source": [
    "Let's now apply this to our entire dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "KhW-TrHJHNj6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reversible_ADJ plug_NOUN works_VERB great_ADJ ._PUNCT\n"
     ]
    }
   ],
   "source": [
    "X_train_annotated = add_pos_tags(X_train)\n",
    "X_test_annotated = add_pos_tags(X_test)\n",
    "\n",
    "vectorizer = CountVectorizer()\n",
    "vectorizer.fit(X_train_annotated)\n",
    "\n",
    "X_train_vec = vectorizer.transform(X_train_annotated)\n",
    "X_test_vec = vectorizer.transform(X_test_annotated)\n",
    "\n",
    "print(X_train_annotated[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "btnG3mMoHTaM"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training accuracy: 0.9816666666666667\n",
      "Testing accuracy: 0.8566666666666667\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "model = LogisticRegression(solver='liblinear')\n",
    "model.fit(X_train_vec, y_train)\n",
    "\n",
    "print('Training accuracy: {}'.format(model.score(X_train_vec, y_train)))\n",
    "print('Testing accuracy: {}'.format(model.score(X_test_vec, y_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "YhO3OKgoudYt"
   },
   "source": [
    "Our training accuracy really went up, but our testing accuracy went down. We are overfitting even more now.\n",
    "\n",
    "This isn't much of a surprise, but is interesting to see that adding even more context (features) can allow a model to fit even tighter than can be done with raw data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "w9Clq0KWL2dD"
   },
   "source": [
    "## Sequential Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "f8DaPLm2tki9"
   },
   "source": [
    "Much of the meaning of language depends on the order of words: \"That movie was not really good\" is not quite the same as \"That movie was really not good.\" For more complicated NLP tasks, a bag-of-words approach does not capture enough useful information. In this section we will instead work with a Recurrent Neural Network (RNN) model, which is specifically designed to capture information about the order of sequences. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "GnTNKN6-IDf6"
   },
   "source": [
    "### Preprocessing\n",
    "\n",
    "We can't use `CountVectorizer` here, so we will need to do some slightly different preprocessing. We can first use the `keras` `Tokenizer` to learn a vocabulary, and then transform each review into a list of indices. Note that we will not include part-of-speech information for this model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "LfcdK_js_dQd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reversible plug works great.\n",
      "[1937, 364, 99, 21]\n"
     ]
    }
   ],
   "source": [
    "from tensorflow import keras\n",
    "\n",
    "tokenizer = keras.preprocessing.text.Tokenizer()\n",
    "tokenizer.fit_on_texts(X_train)\n",
    "\n",
    "X_train_tokenized = tokenizer.texts_to_sequences(X_train)\n",
    "X_test_tokenized = tokenizer.texts_to_sequences(X_test)\n",
    "\n",
    "print(X_train.iloc[0])\n",
    "print(X_train_tokenized[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "MpjwSl-cXioW"
   },
   "source": [
    "We need to pad our input so all vectors have the same length. A quick histogram of review lengths shows that almost all reviews have fewer than 100 words. Let's take a closer look at the distribution of lengths."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "wxrSvmFVWnes"
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYgAAAD4CAYAAAD2FnFTAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8qNh9FAAAACXBIWXMAAAsTAAALEwEAmpwYAAARhklEQVR4nO3df4yl1V3H8ffH3UJ/KVVYG7pLnDWsNkOjbZ2sVBujRWUR4/oHxCX+IIa4fwhKTU2zq5EoCQkkpqgRGomgiE0XXGucAIq10D/6RxeGgoXddXWEVXalMgVK1QRw8Osf90Bux7PMZefuXnbm/Uom+zznOee55zxw72eeH/dMqgpJkpb6pkl3QJL05mRASJK6DAhJUpcBIUnqMiAkSV3rJ92BN+Kss86qqampSXdDkk4ZDz/88FerasPxtD2lAmJqaoq5ublJd0OSThlJ/vV423qJSZLUZUBIkroMCElSlwEhSeoyICRJXQaEJKnLgJAkdRkQkqQuA0KS1HVKfZN6JaZ23TOR1z18/cUTeV1JWinPICRJXQaEJKnLgJAkdRkQkqQuA0KS1DVSQCTZluRQkvkkuzrbT09yZ9u+L8lUKz8zyQNJ/ivJHy5p831JHmtt/iBJxjIiSdJYLBsQSdYBNwEXAdPAZUmml1S7Ani+qs4FbgRuaOUvAr8F/Hpn158EfgnY0n62Hc8AJEknxihnEFuB+ap6oqpeBvYA25fU2Q7c3pb3AhckSVX9d1V9gUFQvCbJ2cC3VNUXq6qAPwN+egXjkCSN2SgBsRF4amj9SCvr1qmqReAF4Mxl9nlkmX0CkGRnkrkkcwsLCyN0V5I0Dm/6m9RVdUtVzVTVzIYNx/V3tyVJx2GUgDgKnDO0vqmVdeskWQ+cATy7zD43LbNPSdIEjRIQDwFbkmxOchqwA5hdUmcWuLwtXwLc3+4tdFXV08DXk5zfnl76BeCv33DvJUknzLKT9VXVYpKrgPuAdcBtVbU/ybXAXFXNArcCdySZB55jECIAJDkMfAtwWpKfBn68qg4Avwz8KfA24G/ajyTpTWKk2Vyr6l7g3iVl1wwtvwhceoy2U8conwPeN2pHJUkn15v+JrUkaTIMCElSlwEhSeoyICRJXQaEJKnLgJAkdRkQkqQuA0KS1GVASJK6DAhJUpcBIUnqMiAkSV0GhCSpy4CQJHUZEJKkLgNCktRlQEiSugwISVKXASFJ6jIgJEldBoQkqcuAkCR1GRCSpC4DQpLUZUBIkroMCElSlwEhSeoyICRJXQaEJKnLgJAkdY0UEEm2JTmUZD7Jrs7205Pc2bbvSzI1tG13Kz+U5MKh8l9Lsj/J40k+neStYxmRJGkslg2IJOuAm4CLgGngsiTTS6pdATxfVecCNwI3tLbTwA7gPGAbcHOSdUk2Ar8KzFTV+4B1rZ4k6U1ilDOIrcB8VT1RVS8De4DtS+psB25vy3uBC5Kkle+pqpeq6klgvu0PYD3wtiTrgbcD/76yoUiSxmmUgNgIPDW0fqSVdetU1SLwAnDmsdpW1VHgd4F/A54GXqiqvzueAUiSToyJ3KRO8q0Mzi42A+8B3pHk545Rd2eSuSRzCwsLJ7ObkrSmjRIQR4FzhtY3tbJunXbJ6Azg2ddp+6PAk1W1UFX/A3wG+IHei1fVLVU1U1UzGzZsGKG7kqRxGCUgHgK2JNmc5DQGN5Nnl9SZBS5vy5cA91dVtfId7SmnzcAW4EEGl5bOT/L2dq/iAuDgyocjSRqX9ctVqKrFJFcB9zF42ui2qtqf5FpgrqpmgVuBO5LMA8/Rnkhq9e4CDgCLwJVV9QqwL8le4Eut/BHglvEPT5J0vDL4Rf/UMDMzU3Nzc8fVdmrXPWPuzWgOX3/xRF5XkgCSPFxVM8fT1m9SS5K6DAhJUpcBIUnqMiAkSV0GhCSpy4CQJHUZEJKkLgNCktRlQEiSugwISVKXASFJ6jIgJEldBoQkqcuAkCR1GRCSpC4DQpLUtexflNPKTOoPFYF/rEjSyngGIUnqMiAkSV0GhCSpy4CQJHUZEJKkLgNCktRlQEiSugwISVKXASFJ6jIgJEldBoQkqcuAkCR1GRCSpC4DQpLUNVJAJNmW5FCS+SS7OttPT3Jn274vydTQtt2t/FCSC4fK35Vkb5J/THIwyYfGMiJJ0lgsGxBJ1gE3ARcB08BlSaaXVLsCeL6qzgVuBG5obaeBHcB5wDbg5rY/gN8H/raq3gt8L3Bw5cORJI3LKGcQW4H5qnqiql4G9gDbl9TZDtzelvcCFyRJK99TVS9V1ZPAPLA1yRnADwG3AlTVy1X1tRWPRpI0NqMExEbgqaH1I62sW6eqFoEXgDNfp+1mYAH4kySPJPnjJO/ovXiSnUnmkswtLCyM0F1J0jhM6ib1euCDwCer6gPAfwP/794GQFXdUlUzVTWzYcOGk9lHSVrTRgmIo8A5Q+ubWlm3TpL1wBnAs6/T9ghwpKr2tfK9DAJDkvQmMUpAPARsSbI5yWkMbjrPLqkzC1zeli8B7q+qauU72lNOm4EtwINV9RXgqSTf3dpcABxY4VgkSWO0frkKVbWY5CrgPmAdcFtV7U9yLTBXVbMMbjbfkWQeeI5BiNDq3cXgw38RuLKqXmm7/hXgUy10ngB+ccxjkyStwLIBAVBV9wL3Lim7Zmj5ReDSY7S9DriuU/4oMPMG+ipJOon8JrUkqcuAkCR1GRCSpC4DQpLUZUBIkroMCElSlwEhSeoyICRJXQaEJKnLgJAkdRkQkqQuA0KS1GVASJK6DAhJUpcBIUnqMiAkSV0GhCSpy4CQJHUZEJKkLgNCktRlQEiSugwISVKXASFJ6jIgJEldBoQkqcuAkCR1GRCSpC4DQpLUZUBIkroMCElSlwEhSeoaKSCSbEtyKMl8kl2d7acnubNt35dkamjb7lZ+KMmFS9qtS/JIkrtXPBJJ0lgtGxBJ1gE3ARcB08BlSaaXVLsCeL6qzgVuBG5obaeBHcB5wDbg5ra/V10NHFzpICRJ4zfKGcRWYL6qnqiql4E9wPYldbYDt7flvcAFSdLK91TVS1X1JDDf9keSTcDFwB+vfBiSpHEbJSA2Ak8NrR9pZd06VbUIvACcuUzb3wM+Dvzv6714kp1J5pLMLSwsjNBdSdI4TOQmdZKfBJ6pqoeXq1tVt1TVTFXNbNiw4ST0TpIEowXEUeCcofVNraxbJ8l64Azg2ddp+4PATyU5zOCS1UeS/Plx9F+SdIKMEhAPAVuSbE5yGoObzrNL6swCl7flS4D7q6pa+Y72lNNmYAvwYFXtrqpNVTXV9nd/Vf3cGMYjSRqT9ctVqKrFJFcB9wHrgNuqan+Sa4G5qpoFbgXuSDIPPMfgQ59W7y7gALAIXFlVr5ygsUiSxmjZgACoqnuBe5eUXTO0/CJw6THaXgdc9zr7/jzw+VH6IUk6efwmtSSpy4CQJHUZEJKkLgNCktRlQEiSugwISVKXASFJ6jIgJEldBoQkqcuAkCR1GRCSpC4DQpLUZUBIkroMCElSlwEhSeoa6e9B6NQ0teueibzu4esvnsjrShovzyAkSV0GhCSpy4CQJHUZEJKkLgNCktRlQEiSugwISVKXASFJ6jIgJEldBoQkqcuAkCR1GRCSpC4DQpLUZUBIkrpGCogk25IcSjKfZFdn++lJ7mzb9yWZGtq2u5UfSnJhKzsnyQNJDiTZn+TqsY1IkjQWywZEknXATcBFwDRwWZLpJdWuAJ6vqnOBG4EbWttpYAdwHrANuLntbxH4WFVNA+cDV3b2KUmaoFHOILYC81X1RFW9DOwBti+psx24vS3vBS5Ikla+p6peqqongXlga1U9XVVfAqiq/wQOAhtXPhxJ0riMEhAbgaeG1o/w/z/MX6tTVYvAC8CZo7Rtl6M+AOzrvXiSnUnmkswtLCyM0F1J0jhM9CZ1kncCfwl8tKq+3qtTVbdU1UxVzWzYsOHkdlCS1rBRAuIocM7Q+qZW1q2TZD1wBvDs67VN8hYG4fCpqvrM8XReknTijBIQDwFbkmxOchqDm86zS+rMApe35UuA+6uqWvmO9pTTZmAL8GC7P3ErcLCqPjGOgUiSxmv9chWqajHJVcB9wDrgtqran+RaYK6qZhl82N+RZB54jkGI0OrdBRxg8OTSlVX1SpIPAz8PPJbk0fZSv1FV9455fJKk47RsQAC0D+57l5RdM7T8InDpMdpeB1y3pOwLQN5oZyVJJ4/fpJYkdRkQkqQuA0KS1GVASJK6DAhJUpcBIUnqMiAkSV0GhCSpy4CQJHUZEJKkLgNCktRlQEiSugwISVKXASFJ6jIgJEldBoQkqcuAkCR1GRCSpC4DQpLUZUBIkroMCElSlwEhSeoyICRJXesn3QGtPlO77pnYax++/uKJvba02ngGIUnqMiAkSV0GhCSpy4CQJHUZEJKkLgNCktTlY65aVSb1iK2P12o1GukMIsm2JIeSzCfZ1dl+epI72/Z9SaaGtu1u5YeSXDjqPiVJk7XsGUSSdcBNwI8BR4CHksxW1YGhalcAz1fVuUl2ADcAP5NkGtgBnAe8B/j7JN/V2iy3T+mU4ZmLVqNRLjFtBear6gmAJHuA7cDwh/l24Lfb8l7gD5Okle+pqpeAJ5PMt/0xwj4lLcNvretEGiUgNgJPDa0fAb7/WHWqajHJC8CZrfyLS9pubMvL7ROAJDuBnW31v5IcGqHPZwFfHaHearbWj8FaHz+c4GOQG07UnsfG/wcGx+A7jrfxm/4mdVXdAtzyRtokmauqmRPUpVPCWj8Ga3384DFY6+OH147B1PG2H+Um9VHgnKH1Ta2sWyfJeuAM4NnXaTvKPiVJEzRKQDwEbEmyOclpDG46zy6pMwtc3pYvAe6vqmrlO9pTTpuBLcCDI+5TkjRBy15iavcUrgLuA9YBt1XV/iTXAnNVNQvcCtzRbkI/x+ADn1bvLgY3nxeBK6vqFYDePsc4rjd0SWqVWuvHYK2PHzwGa338sMJjkMEv+pIkfSOn2pAkdRkQkqSuVRcQa2EKjyS3JXkmyeNDZd+W5LNJ/rn9+62tPEn+oB2PLyf54OR6Pj5JzknyQJIDSfYnubqVr4njkOStSR5M8g9t/L/Tyje36W7m2/Q3p7XyY06HcypLsi7JI0nubutrbfyHkzyW5NEkc61sbO+BVRUQQ9OCXARMA5e16T5Wmz8Fti0p2wV8rqq2AJ9r6zA4Flvaz07gkyepjyfaIvCxqpoGzgeubP+t18pxeAn4SFV9L/B+YFuS8xlMc3NjVZ0LPM9gGhwYmg4HuLHVWw2uBg4Ora+18QP8SFW9f+g7H+N7D1TVqvkBPgTcN7S+G9g96X6doLFOAY8PrR8Czm7LZwOH2vIfAZf16q2mH+CvGcztteaOA/B24EsMZiP4KrC+lb/2fmDwxOCH2vL6Vi+T7vsKx72pfQB+BLgbyFoafxvLYeCsJWVjew+sqjMI+tOCbDxG3dXm3VX1dFv+CvDutrzqj0m7XPABYB9r6Di0yyuPAs8AnwX+BfhaVS22KsNj/IbpcIBXp8M5lf0e8HHgf9v6mayt8QMU8HdJHm7TEsEY3wNv+qk29MZVVSVZE88vJ3kn8JfAR6vq64M5IgdW+3GowXeK3p/kXcBfAe+dbI9OniQ/CTxTVQ8n+eEJd2eSPlxVR5N8O/DZJP84vHGl74HVdgaxlqfw+I8kZwO0f59p5av2mCR5C4Nw+FRVfaYVr7njUFVfAx5gcEnlXW26G/jGMR5rOpxT1Q8CP5XkMLCHwWWm32ftjB+Aqjra/n2GwS8JWxnje2C1BcRansJjeLqTyxlck3+1/BfaEwznAy8MnX6esjI4VbgVOFhVnxjatCaOQ5IN7cyBJG9jcP/lIIOguKRVWzr+3nQ4p6Sq2l1Vm2owEd0OBuP5WdbI+AGSvCPJN7+6DPw48DjjfA9M+ibLCbhp8xPAPzG4Hvubk+7PCRrjp4Gngf9hcB3xCgbXUz8H/DPw98C3tbph8GTXvwCPATOT7v+YjsGHGVx//TLwaPv5ibVyHIDvAR5p438cuKaVfyeD+c7mgb8ATm/lb23r8237d056DGM8Fj8M3L3Wxt/G+g/tZ/+rn3fjfA841YYkqWu1XWKSJI2JASFJ6jIgJEldBoQkqcuAkCR1GRCSpC4DQpLU9X/6NOvk08M/cgAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "review_lengths = [len(review) for review in X_train]\n",
    "plt.hist(review_lengths, density=True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "TD1eYtMgKm_z"
   },
   "source": [
    "Almost all reviews have fewer than 50 words! Therefore, we will pad to a maximum review length of 50."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "j2-Vzg7YpILj"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1937  364   99   21    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0]\n"
     ]
    }
   ],
   "source": [
    "maxlen = 50\n",
    "\n",
    "X_train_padded = keras.preprocessing.sequence.pad_sequences(\n",
    "    X_train_tokenized, padding='post', maxlen=maxlen)\n",
    "X_test_padded = keras.preprocessing.sequence.pad_sequences(\n",
    "    X_test_tokenized, padding='post', maxlen=maxlen)\n",
    "\n",
    "print(X_train_padded[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "i9g65GfQ920_"
   },
   "source": [
    "### Pre-Trained Word Embeddings\n",
    "\n",
    "Word embeddings are foundational to most NLP tasks. It's common to experiment with embeddings, feature extraction, or a combination of both to determine what works best with your specific data and problem.\n",
    "\n",
    "In practice, instead of training our own embeddings, we can often take advantage of existing embeddings that have already been trained. This is especially useful when we have a small dataset and want or need the richer meaning that comes from embeddings trained on a larger dataset. \n",
    "\n",
    "There are a variety of extensively pre-trained word embeddings. One of the most powerful and widely-used is [GloVe (Global Vectors for Word Representation)](https://nlp.stanford.edu/projects/glove/). Luckily for us, the spaCy model we downloaded is already integrated with 300-dimensional GloVe embeddings. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Knn-312W921J"
   },
   "source": [
    "All we need to do is load these embeddings into an `embedding_matrix` so that each word index properly matches with the words in our dataset. We can access the `tokenizer`'s vocabulary using `.word_index`.\n",
    "\n",
    "*Note: This may take a few minutes to run.*\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "9Y0-sVSeuCW6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4620, 300)\n",
      "CPU times: user 16.7 s, sys: 48.7 ms, total: 16.7 s\n",
      "Wall time: 16.8 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Include an extra index for the \"<PAD>\" token.\n",
    "vocab_size = len(tokenizer.word_index) + 1\n",
    "embedding_dim = 300\n",
    "embedding_matrix = np.zeros((vocab_size, embedding_dim))\n",
    "\n",
    "for word, i in tokenizer.word_index.items():\n",
    "  token = spacy_model(word)[0]\n",
    "  # Make sure spaCy has an embedding for this token.\n",
    "  if not token.is_oov:\n",
    "    embedding_matrix[i] = token.vector\n",
    "\n",
    "print(embedding_matrix.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "NRgcHn2WMxwH"
   },
   "source": [
    "Loading the embeddings may take a little while to run. When it's done we'll have an `embedding_matrix` where each word index corresponds to a 300-dimensional GloVe vector. We can load this into an `Embedding` layer to train a model or visualize the embeddings.\n",
    "\n",
    "Also note that we have slightly more tokens now than from using `CountVectorizer`. This means that Keras' `Tokenizer` splits sentences into tokens using slightly different rules."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "v2XyVjq4KBxv"
   },
   "source": [
    "### RNN Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "JFw-Fd4YFZZR"
   },
   "source": [
    "This model will have three layers:\n",
    "\n",
    "1. `Embedding`\n",
    "\n",
    "   We initialize its weights using the `embedding_matrix` of pre-trained GloVe embeddings. We set `trainable=False` to prevent the weights from being updated during training. You can keep `trainable=True` to allow for additional training, or \"fine-tuning\", of these weights. We also set `mask_zero=True` to ensure we do not train parameters based on the `\"<PAD>\"` tokens.\n",
    "\n",
    "2. `LSTM` (Long Short-Term Memory)\n",
    "\n",
    "   This is a type of RNN architecture that is especially good at handling long sequences of information. This layer takes input of dimensions `(batch size, maxlen, embedding dimension)` and returns output of dimensions `(batch size, 64)`. A larger output size means a more complex model; we have chosen 64 after tuning based on model performance.\n",
    "\n",
    "3. `Dense`\n",
    "\n",
    "   A final layer to return a prediction of either positive or negative sentiment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "GXGR-ledW0KK"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-07-03 00:25:12.659454: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding (Embedding)       (None, None, 300)         1386000   \n",
      "                                                                 \n",
      " lstm (LSTM)                 (None, 64)                93440     \n",
      "                                                                 \n",
      " dense (Dense)               (None, 1)                 65        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1,479,505\n",
      "Trainable params: 93,505\n",
      "Non-trainable params: 1,386,000\n",
      "_________________________________________________________________\n",
      "CPU times: user 782 ms, sys: 30.4 ms, total: 813 ms\n",
      "Wall time: 787 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "model = keras.Sequential([\n",
    "  keras.layers.Embedding(\n",
    "    vocab_size,\n",
    "    embedding_dim,\n",
    "    weights=[embedding_matrix],\n",
    "    trainable=False,\n",
    "    mask_zero=True\n",
    "  ),\n",
    "  keras.layers.LSTM(64),\n",
    "  keras.layers.Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "VbPiVaMfRnZm"
   },
   "source": [
    "We will train this model for 10 epochs since it is slower to train per epoch and reaches high training accuracy after 10 epochs. We use a batch size of 64 based on hyperparameter tuning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Ios8hOAOr6dy"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "38/38 [==============================] - 4s 31ms/step - loss: 0.6323 - accuracy: 0.6521\n",
      "Epoch 2/10\n",
      "38/38 [==============================] - 1s 35ms/step - loss: 0.5719 - accuracy: 0.6992\n",
      "Epoch 3/10\n",
      "38/38 [==============================] - 2s 44ms/step - loss: 0.5292 - accuracy: 0.7362\n",
      "Epoch 4/10\n",
      "38/38 [==============================] - 2s 56ms/step - loss: 0.4899 - accuracy: 0.7633\n",
      "Epoch 5/10\n",
      "38/38 [==============================] - 3s 67ms/step - loss: 0.4627 - accuracy: 0.7792\n",
      "Epoch 6/10\n",
      "38/38 [==============================] - 2s 64ms/step - loss: 0.4377 - accuracy: 0.7946\n",
      "Epoch 7/10\n",
      "38/38 [==============================] - 2s 61ms/step - loss: 0.4152 - accuracy: 0.8108\n",
      "Epoch 8/10\n",
      "38/38 [==============================] - 2s 50ms/step - loss: 0.3806 - accuracy: 0.8238\n",
      "Epoch 9/10\n",
      "38/38 [==============================] - 2s 61ms/step - loss: 0.3495 - accuracy: 0.8438\n",
      "Epoch 10/10\n",
      "38/38 [==============================] - 2s 57ms/step - loss: 0.3189 - accuracy: 0.8654\n",
      "CPU times: user 50.1 s, sys: 37.3 s, total: 1min 27s\n",
      "Wall time: 23.1 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "model.compile(\n",
    "  loss='binary_crossentropy',\n",
    "  optimizer='adam',\n",
    "  metrics=['accuracy']\n",
    ")\n",
    "\n",
    "history = model.fit(\n",
    "    X_train_padded,\n",
    "    y_train,\n",
    "    epochs=10,\n",
    "    batch_size=64\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "q-xT4yVcBQgH"
   },
   "source": [
    "And finally, we can evaluate the accuracy of the model on our test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "1ZjxVWCUr7l7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19/19 [==============================] - 2s 25ms/step - loss: 0.5938 - accuracy: 0.7017\n",
      "Test accuracy: 0.7016666531562805\n"
     ]
    }
   ],
   "source": [
    "loss, acc = model.evaluate(X_test_padded, y_test)\n",
    "print('Test accuracy: {}'.format(acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "rpSjxlrOeonH"
   },
   "source": [
    "Note that the final testing set accuracy is not significantly higher than that of our Logistic Regression model. We are using a complex model on a small dataset, which is prone to overfitting. You can usually achieve more generalizable results with a larger dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "SdqWCg3eLldl"
   },
   "source": [
    "# Exercise 1: A Tale of Two Authors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "7UzN5qJnJ8lG"
   },
   "source": [
    "In this exercise we will create a model that can determine if a paragraph was written by Jane Austen or Charles Dickens. We'll use a [dataset containing the works of the two authors](https://www.kaggle.com/joshmcadams/jane-austen-and-charles-dickens) sourced from [Project Gutenberg](https://www.gutenberg.org/).\n",
    "\n",
    "Your task is to download the data and build a classifier that can distinguish between the works of the two authors using techniques covered earlier in this lab. Experiment with different types of models, and see if you can build one that trains and generalizes well.\n",
    "\n",
    "Use as many text and code cells as you need. Be sure to explain your work."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "eEVrX5IuZ0Vk"
   },
   "source": [
    "## **Student Solution**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "jbFKVbMWZ2ZC"
   },
   "outputs": [],
   "source": [
    "# Your code goes here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "RATiicNtZ33w"
   },
   "source": [
    "---"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [
    "copyright",
    "Dex6HBiIZ4yh"
   ],
   "include_colab_link": true,
   "name": "Natural Language Processing",
   "private_outputs": true,
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
